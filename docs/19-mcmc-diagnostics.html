<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>MCMC diagnostics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Math 315: Bayesian Statistics" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MCMC diagnostics
### Math 315: Bayesian Statistics

---





# Tuning MCMC

.large[
Three main decisions:

- Selecting the initial values

- Determining if/when the chain(s) has converged

- Selecting the number of samples needed to approximate
]

---

# Initial values

.large[
- The algorithm will eventually converge no matter what
initial values you select

- Choosing good starting values will speed up convergence

- It is important to try a few initial values to verify they all give
the same result

- Usually 3-5 separate chains is sufficient

.content-box-yellow[
**Common strategies**:

1. Select good initial values using method of
moments or MLE

2. Purposely pick bad but different initial values for
each chain to check convergence
]
]

---

# Convergence diagnostics

.large[

.content-box-yellow[
Don't assume that your Markov chain converged to the desired stationary distribution just because "it worked."
]


A **"good" Markov chain** should be:

- **Stationary**: draws should within the posterior (stable path around the center of gravity)

- **Well mixed**: 

    + successive draws should not be highly correlated
    
    + each chain should target the same stationary distribution

]

---

# Convergence diagnostics
.large[
The **coda** R package has dozens of diagnostics


```r
library(coda)
```


Did my chains converge?

- Geweke

- Gelman-Rubin


Did I run the sampler long enough after convergence?

- Effective sample size

- Standard errors for the posterior mean estimate
]

---
class: inverse

# Your turn

.large[.bold[With a neighbor, decide whether each chain is well mixed]]

&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-2-1.svg" width="100%" /&gt;


---
class: inverse

# Your turn

.large[.bold[With a neighbor, decide whether each chain converged]]

&lt;img src="19-mcmc-diagnostics_files/figure-html/yourturn-chain1-1.svg" width="100%" /&gt;


---

# Geweke diagnostic

.large[
- **Idea**: a two-sample test comparing the mean of a chain between batches at the beginning versus the end 

- By default, JAGS compares the first 10% with the last 50%

- Test statistic is a Z-score with the standard errors
adjusted for autocorrelation
]

&lt;img src="19-mcmc-diagnostics_files/figure-html/yourturn-chain1-1.svg" width="95%" /&gt;

---

# Geweke diagnostic

.pull-left[

&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-3-1.svg" width="85%" /&gt;


```r
geweke.diag(bad_samples[[1]])
```

```
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  mu[1]  mu[2] 
## -1.475  1.559
```


]

.pull-right[

&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-5-1.svg" width="85%" /&gt;


```r
geweke.diag(good_samples[[1]])
```

```
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  mu[1]  mu[2] 
##  1.648 -1.262
```


]

---

# Gelman-Rubin diagnostic
  
.large[
- Effective convergence of Markov chain simulation has
been reached when inferences for quantities of interest do not depend
on the starting point of the simulations.

- If we run multiple chains, we hope that all chains give same result

- Gelman and Rubin (1992) proposed (essentially) an ANOVA test of whether the chains have the same mean

- `\(R_j\)` is scaled and approaches 1 from above

    + `\(R_j = 1 \Rightarrow\)` perfect convergence

    + `\(R_j \ge 1.1 \Rightarrow\)` red flag
]

---

# Gelman-Rubin diagnostic

.pull-left[
&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-7-1.svg" width="85%" /&gt;


```r
gelman.diag(mu1_bad)
```

```
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.05       1.08
```


]

.pull-right[

&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-9-1.svg" width="85%" /&gt;


```r
gelman.diag(mu1_good)
```

```
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1          1
```


]

---

# Autocorrelation

.large[
**Definition:**  the correlation between samples `\(h\)` iterations apart

`$$\rho(h) = {\rm Corr}(\theta_j^{(s)}, \theta_j^{(s-h)})$$`

**So what?**&lt;br&gt; A chain with high correlation is said to exhibit **poor mixing**

]

---

# Autocorrelation

.pull-left[
&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-11-1.svg" width="80%" /&gt;

]

.pull-right[
&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-12-1.svg" width="80%" /&gt;
]


---

# Autocorrelation

.large[
- Lower values are better, but if the chains are long enough
even large values can be OK

- **Thinning** the Markov chain means keeping only every kth draw, where k is chosen so that the autocorrelation is small

- `thin` argument to `coda.samples()` implements this

- This is always less efficient than using all samples, but can
save memory
]

---

# Effective sample size

.large[
- Highly correlated samples have less information than
independent samples

- `\(S=\)` \# MCMC samples after burn in


- **Effective samples size (ESS)**

`$$ESS_k = S \Big/ \left(1 + 2 \displaystyle \sum_{k=1}^\infty \rho(k)\right)$$`

- ESS = "equivalent number of independent observations"

- Should be at least a few thousand for all parameters
]

---

# Effective sample size

.pull-left[
&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-13-1.svg" width="90%" /&gt;


```r
# ESS for single chain of mu1
# n.iter = 5000
effectiveSize(mu1_bad[[1]])  
```

```
##     var1 
## 5.624371
```


]

.pull-right[
&lt;img src="19-mcmc-diagnostics_files/figure-html/unnamed-chunk-15-1.svg" width="90%" /&gt;


```r
# ESS for single chain of mu1
# n.iter = 5000
effectiveSize(mu1_good[[1]])  
```

```
##     var1 
## 2247.772
```
]

---

# Standard errors of posterior mean estimates

.large[
- Assuming independence the standard error is

    `$$\text{Naive SE} = \dfrac{s}{\sqrt{S}}$$`

    where `\(s =\)` sample SD
    
- A more realistic standard error is

`$$\text{Time-series SE} = \dfrac{s}{\sqrt{ESS}}$$`

-  If the SE is too large, rerun the MCMC algorithm for a larger number of samples
]

---

# Standard errors of posterior mean estimates


```r
summary(bad_samples)  
```

```
## 
## Iterations = 2001:7000
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean    SD Naive SE Time-series SE
## mu[1]  1.128 20.30   0.1658          5.254
## mu[2] -1.707 20.29   0.1656          5.205
## 
## 2. Quantiles for each variable:
## 
##         2.5%    25%    50%   75% 97.5%
## mu[1] -42.64 -11.85  2.433 16.57 35.89
## mu[2] -36.49 -17.22 -3.049 11.42 41.89
```


---

# Standard errors of posterior mean estimates



```r
summary(good_samples)    
```

```
## 
## Iterations = 2001:7000
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean     SD Naive SE Time-series SE
## mu[1] -0.575 1.2711 0.010378       0.015457
## mu[2]  2.252 0.3244 0.002649       0.003399
## 
## 2. Quantiles for each variable:
## 
##         2.5%    25%     50%    75% 97.5%
## mu[1] -3.612 -1.252 -0.3645 0.3278 1.326
## mu[2]  1.574  2.041  2.2700 2.4786 2.840
```



---

# What to do if the chains haven't converged?

.large[
- Increase the number of iterations

- Tune the M-H candidate distribution

- Use better initial values

- Use a more advanced algorithm

- Simplify/reparameterize the model

- Use (more) informative priors

- Run a simulation study

]

---

# What to do for massive datsets?

.large[
- MAP estimation

- Bayesian CLT

- Variational Bayesian computing: Approximates the posterior by assuming the posterior is independent across parameters (fast, but questionable statistical properties)

-  Parallel computing: MCMC is inherently sequential, but often some steps can be done in parallel, e.g., onerous likelihood computations

- Divide and Conquer: Split the data into batches and
analysis them in parallel, and then carefully combine the
result of the batch analyses
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
