<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>MCMC software</title>
    <meta charset="utf-8" />
    <meta name="author" content="Math 315: Bayesian Statistics" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MCMC software
### Math 315: Bayesian Statistics

---




# Summary of MCMC

.large[
- With the combination of Gibbs and Metropolis-Hastings
sampling we can fit virtually any model

- In some cases Bayesian computing is actually preferable
to maximum likelihood analysis

- In most cases Bayesian computing is slower

- However, it is often worth the wait for
improved uncertainty quantification and interpretability

- In all cases it is important to carefully monitor convergence 
]

.footnote[Slide adapted from Brian Reich]

---

# Why Just Another Gibbs Sampler (JAGS)?

.large[
- You can fit virtually any model

- You can call JAGS from R, allowing for plotting and
data manipulation in R

- It runs on all platforms: LINUX, Mac, Windows

- There is a lot of help online

- R has many built in packages for convergence diagnostics
]

.footnote[Slide adapted from Brian Reich]

---

# How does JAGS work?

.large[
- You specify the model by declaring the likelihood and priors

- JAGS then sets up the MCMC sampler
    + e.g., works out the full conditional distributions for all parameters

- It returns MCMC samples in a matrix or array

- It also automatically produces posterior summaries like
means, credible sets, and convergence diagnostics

- Userâ€™s manual: http://people.stat.sc.edu/hansont/stat740/jags_user_manual.pdf
]

.footnote[Slide adapted from Brian Reich]

---

# Steps for any analysis using JAGS

.large[
1. Specify the model as a string

2. Compile the model using `jags.model()`

3. Draw burn-in samples using `update()`

4. Draw posterior samples using `coda.samples()`

5. Inspect/summarize the results using the `plot()` and `summary()`
]

---

# Example: Rocket launches

.large[
`\(Y_i =\)` 0 or 1 (failure/success)

.bold[Likelihood:] `\(Y_i \overset{\rm iid}{\sim}{\rm Bernoulli} (\theta)\)`

.bold[Prior:] Elicitation leads to uniform on (0.1, 0.9)

.bold[Posterior:]

`$$p(\theta | y) \propto \begin{cases}
\theta^3 (1-\theta)^8 &amp; \text{if } 0.1 &lt; \theta &lt; 0.9\\
0 &amp; \text{otherwise.}\end{cases}$$`
]

---

# 0. Load rjags and data


```r
library(rjags)

# Load data
launches &lt;- read.table("https://www4.stat.ncsu.edu/~wilson/BR/table21.txt", header = TRUE)
y &lt;- launches$Outcome
n &lt;- nrow(launches)
```

---

# 1. Specify the model as a string


```r
model_string &lt;- textConnection("model{

  # Likelihood
  for(i in 1:n) {
    y[i] ~ dbern(theta)
  }
  
  # Prior
  theta ~ dunif(a, b)
  
}")
```

.content-box-yellow[
.large[
Be sure to use `textConnection()` if you are not saving this model string as a separate text file.
]
]

---
# 2. Compile the MCMC code


```r
inits &lt;- list(theta = 0.5)
data  &lt;- list(y = y, n = n, a = 0.1, b = 0.9)
model &lt;- jags.model(model_string, data = data, 
                    inits = inits, n.chains = 1)
```

```
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 11
##    Unobserved stochastic nodes: 1
##    Total graph size: 15
## 
## Initializing model
```


.content-box-yellow[
.large[
`inits` and `data` should be **lists**
]
]

---

# 3. Draw burn-in samples 


```r
update(model, 1000)
```

.content-box-yellow[
.large[
Add `progress.bar = "none"` to silence the progress bar in your .Rmd files]
]

---

# 4. Draw posterior samples


```r
samples &lt;- coda.samples(
  model, 
  variable.names = "theta", 
  n.iter = 10000, 
  progress.bar = "none"
)
```

.content-box-yellow[
.large[
`variable.names` expects a character vector with the names of the variables to be monitored]
]


---

# 5. Inspect/summarize the results


```r
summary(samples)
```

```
## 
## Iterations = 2001:12000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       0.314917       0.120378       0.001204       0.001639 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## 0.1228 0.2231 0.3031 0.3927 0.5792
```

---


# 5. Inspect/summarize the results


```r
plot(samples)
```

![](18-jags_files/figure-html/unnamed-chunk-7-1.svg)&lt;!-- --&gt;

---

class: inverse

# Your turn

.Large[.bolder[Work through the examples on the JAGS handout with a neighbor.]]

.Large[.bolder[Check GitHub for a .Rmd template]]

.Large[.bolder[https://github.com/aloy/math315-fall2019]]

---

# Tuning MCMC

.large[
Three main decisions:

- Selecting the initial values

- Determining if/when the chain(s) has converged

- Selecting the number of samples needed to approximate
]

---

# Initial values

.large[
- The algorithm will eventually converge no matter what
initial values you select

- Choosing good starting values will speed up convergence

- It is important to try a few initial values to verify they all give
the same result

- Usually 3-5 separate chains is sufficient

.content-box-yellow[
**Common strategies**:

1. Select good initial values using method of
moments or MLE

2. Purposely pick bad but different initial values for
each chain to check convergence
]
]

---

# Convergence diagnostics

.large[

.content-box-yellow[
Don't assume that your Markov chain converged to the desired stationary distribution just because "it worked."
]


A **"good" Markov chain** should be:

- **Stationary**: draws should within the posterior (stable path around the center of gravity)

- **Well mixed**: 

    + successive draws should not be highly correlated
    
    + each chain should target the same stationary distribution

]

---

# Convergence diagnostics
.large[
The **coda** R package has dozens of diagnostics


```r
library(coda)
```


Did my chains converge?

- Geweke

- Gelman-Rubin


Did I run the sampler long enough after convergence?

- Effective sample size

- Standard errors for the posterior mean estimate
]

---
class: inverse

# Your turn

.large[.bold[With a neighbor, decide whether the chains are well mixed]]

&lt;img src="18-jags_files/figure-html/unnamed-chunk-9-1.svg" width="100%" /&gt;


---
class: inverse

# Your turn

.large[.bold[With a neighbor, decide whether each chain converged]]

&lt;img src="18-jags_files/figure-html/yourturn-chain1-1.svg" width="100%" /&gt;


---

# Geweke diagnostic

.large[
- **Idea**: a two-sample test comparing the mean of a chain between batches at the beginning versus the end 

- By default, JAGS compares the first 10% with the last 50%

- Test statistic is a Z-score with the standard errors
adjusted for autocorrelation
]

&lt;img src="18-jags_files/figure-html/yourturn-chain1-1.svg" width="95%" /&gt;

---

# Geweke diagnostic

.pull-left[

&lt;img src="18-jags_files/figure-html/unnamed-chunk-10-1.svg" width="85%" /&gt;


```r
geweke.diag(bad_samples[[1]])
```

```
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  mu[1]  mu[2] 
## -1.110  1.098
```


]

.pull-right[

&lt;img src="18-jags_files/figure-html/unnamed-chunk-12-1.svg" width="85%" /&gt;


```r
geweke.diag(good_samples[[1]])
```

```
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##   mu[1]   mu[2] 
## 0.08223 0.01925
```


]

---

# Gelman-Rubin diagnostic
  
.large[
- Effective convergence of Markov chain simulation has
been reached when inferences for quantities of interest do not depend
on the starting point of the simulations.

- If we run multiple chains, we hope that all chains give same result

- Gelman and Rubin (1992) proposed (essentially) an ANOVA test of whether the chains have the same mean

- `\(R_j\)` is scaled and approaches 1 from above

    + `\(R_j = 1 \Rightarrow\)` perfect convergence

    + `\(R_j \ge 1.1 \Rightarrow\)` red flag
]

---

# Gelman-Rubin diagnostic

.pull-left[
&lt;img src="18-jags_files/figure-html/unnamed-chunk-14-1.svg" width="85%" /&gt;


```r
gelman.diag(mu1_bad)
```

```
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.25       1.69
```


]

.pull-right[

&lt;img src="18-jags_files/figure-html/unnamed-chunk-16-1.svg" width="85%" /&gt;


```r
gelman.diag(mu1_good)
```

```
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1          1
```


]

---

# Autocorrelation

.large[
**Definition:**  the correlation between samples `\(h\)` iterations apart

`$$\rho(h) = {\rm Corr}(\theta_j^{(s)}, \theta_j^{(s-h)})$$`

**So what?**&lt;br&gt; A chain with high correlation is said to exhibit **poor mixing**

]

---

# Autocorrelation

.pull-left[
&lt;img src="18-jags_files/figure-html/unnamed-chunk-18-1.svg" width="80%" /&gt;

]

.pull-right[
&lt;img src="18-jags_files/figure-html/unnamed-chunk-19-1.svg" width="80%" /&gt;
]


---

# Autocorrelation

.large[
- Lower values are better, but if the chains are long enough
even large values can be OK

- **Thinning** the Markov chain means keeping only every kth draw, where k is chosen so that the autocorrelation is small

- `thin` argument to `coda.samples()` implements this

- This is always less efficient than using all samples, but can
save memory
]

---

# Effective sample size

.large[
- Highly correlated samples have less information than
independent samples

- `\(S=\)` \# MCMC samples after burn in


- **Effective samples size (ESS)**

`$$ESS_k = S \Big/ \left(1 + 2 \displaystyle \sum_{k=1}^\infty \rho(k)\right)$$`

- ESS = "equivalent number of independent observations"

- Should be at least a few thousand for all parameters
]

---

# Effective sample size

.pull-left[
&lt;img src="18-jags_files/figure-html/unnamed-chunk-20-1.svg" width="90%" /&gt;


```r
# ESS for single chain of mu1
# n.iter = 5000
effectiveSize(mu1_bad[[1]])  
```

```
##     var1 
## 4.095937
```


]

.pull-right[
&lt;img src="18-jags_files/figure-html/unnamed-chunk-22-1.svg" width="90%" /&gt;


```r
# ESS for single chain of mu1
# n.iter = 5000
effectiveSize(mu1_good[[1]])  
```

```
##     var1 
## 2067.676
```
]

---

# Standard errors of posterior mean estimates

.large[
- Assuming independence the standard error is

    `$$\text{Naive SE} = \dfrac{s}{\sqrt{S}}$$`

    where `\(s =\)` sample SD
    
- A more realistic standard error is

`$$\text{Time-series SE} = \dfrac{s}{\sqrt{ESS}}$$`

-  If the SE is too large, rerun the MCMC algorithm for a larger number of samples
]

---

# Standard errors of posterior mean estimates


```r
summary(bad_samples)  
```

```
## 
## Iterations = 2001:7000
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean    SD Naive SE Time-series SE
## mu[1] -4.897 22.27   0.1818          5.392
## mu[2]  4.324 22.27   0.1818          5.303
## 
## 2. Quantiles for each variable:
## 
##         2.5%    25%    50%   75% 97.5%
## mu[1] -40.64 -21.89 -7.691 10.03 42.74
## mu[2] -43.24 -10.72  7.152 21.26 40.01
```


---

# Standard errors of posterior mean estimates



```r
summary(good_samples)    
```

```
## 
## Iterations = 2001:7000
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean     SD Naive SE Time-series SE
## mu[1] -0.551 1.2428 0.010147       0.015754
## mu[2]  2.251 0.3235 0.002641       0.003369
## 
## 2. Quantiles for each variable:
## 
##         2.5%    25%     50%    75% 97.5%
## mu[1] -3.470 -1.214 -0.3561 0.3364 1.293
## mu[2]  1.563  2.044  2.2699 2.4751 2.840
```



---

# What to do if the chains haven't converged?

.large[
- Increase the number of iterations

- Tune the M-H candidate distribution

- Use better initial values

- Use a more advanced algorithm

- Simplify/reparameterize the model

- Use (more) informative priors

- Run a simulation study

]

---

# What to do for massive datsets?

.large[
- MAP estimation

- Bayesian CLT

- Variational Bayesian computing: Approximates the posterior by assuming the posterior is independent across parameters (fast, but questionable statistical properties)

-  Parallel computing: MCMC is inherently sequential, but often some steps can be done in parallel, e.g., onerous likelihood computations

- Divide and Conquer: Split the data into batches and
analysis them in parallel, and then carefully combine the
result of the batch analyses
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
